# Advanced Lakehouse Platform - GDPR Compliance Example

Dieses Beispiel demonstriert eine vollstÃ¤ndige Event-Sourcing-basierte Data Lakehouse Platform mit GDPR-Compliance.

## Features

âœ… **GDPR-Compliance**
- Compliance-Level Management (Standard, GDPR, Strict)
- Right-to-be-forgotten Implementation
- 30-Tage-LÃ¶schfristen nach DSGVO
- VollstÃ¤ndige Audit-Trails

âœ… **Event Sourcing**
- VollstÃ¤ndige Event-Historie aller Ã„nderungen
- Event Replay fÃ¼r Aggregate-Rekonstruktion  
- Snapshot-Optimierung bei groÃŸen Event-Streams
- Immutable Event-Log

âœ… **Data Lake Management**
- Dataset-Ingestion mit PII-Tracking
- Compliance-Level-basierte Validierung
- Access Auditing fÃ¼r Compliance-Reports
- Multi-Region Support

## Architektur

```
Command â†’ Aggregate â†’ Event â†’ Event Store
                â†“
           State Update
                â†“
         Query Functions
```

### Domain Model

**Commands:**
- `CreateDataLakeCommand` - Erstellt einen neuen Data Lake
- `IngestDatasetCommand` - Ingesti ein Dataset  
- `RequestGDPRDeletionCommand` - DSGVO LÃ¶schanfrage
- `AuditAccessCommand` - Audit-Logging

**Events:**
- `DataLakeCreatedEvent` - Lake wurde erstellt
- `DatasetIngestedEvent` - Dataset wurde hinzugefÃ¼gt
- `GDPRDeletionRequestedEvent` - LÃ¶schung wurde angefordert
- `AccessAuditedEvent` - Zugriff wurde geloggt

**Aggregate:**
- `DataLakeAggregate` - Verwaltet kompletten Lake-Lifecycle

## AusfÃ¼hrung

```bash
# Basis-Demo
uv run python examples/lakehouse_gdpr_simple.py

# Tests (wenn vorhanden)
uv run pytest examples/test_lakehouse.py -v
```

## Output

```
ðŸ—ï¸  Advanced Lakehouse Platform with GDPR Compliance

1ï¸âƒ£  Creating GDPR-compliant data lake...
   âœ… Lake created: EU Customer Analytics (compliance: gdpr)

2ï¸âƒ£  Ingesting datasets...
   âœ… 2 datasets ingested
   ðŸ“Š Total records: 130,000
   ðŸ”’ PII datasets: ['sales-2024']

3ï¸âƒ£  Auditing data access...
   âœ… 2 access events logged

4ï¸âƒ£  Processing GDPR deletion request...
   âœ… Deletion request created: del-customer-42-1234567890
   â° Deadline: 2026-02-02
   ðŸ“ Status: pending

5ï¸âƒ£  Persisting events to event store...
   âœ… 5 events saved

6ï¸âƒ£  Reconstructing aggregate from events...
   âœ… Aggregate reconstructed from 5 events
   ðŸ“Š Datasets: 2
   ðŸ” Access logs: 2
   ðŸ—‘ï¸  Deletion requests: 1

7ï¸âƒ£  Creating snapshot for optimization...
   âœ… Snapshot created at version 5

8ï¸âƒ£  Compliance Report:
   â€¢ Lake: EU Customer Analytics
   â€¢ Compliance: GDPR
   â€¢ Region: eu-west-1
   â€¢ Total datasets: 2
   â€¢ PII datasets: 1
   â€¢ Pending deletions: 1
   â€¢ Access events: 2
   â€¢ Event version: 5

âœ… GDPR-compliant lakehouse operational!
```

## GDPR-Compliance Features

### 1. Compliance Levels

```python
class ComplianceLevel(str, Enum):
    STANDARD = "standard"  # Basis-Compliance
    GDPR = "gdpr"          # DSGVO-Compliance
    STRICT = "strict"      # Erweiterte Compliance
```

### 2. Right-to-be-Forgotten

```python
lake.handle_gdpr_deletion(RequestGDPRDeletionCommand(
    lake_id="lake-001",
    subject_id="customer-42",
    reason="User requested right to be forgotten",
    requested_by="support-agent"
))
```

- Automatische 30-Tage-Deadline
- Status-Tracking (pending â†’ completed)
- VollstÃ¤ndiger Audit-Trail

### 3. PII-Tracking

```python
lake.handle_ingest_dataset(IngestDatasetCommand(
    lake_id="lake-001",
    dataset_id="customer-data",
    source="crm_export",
    record_count=50000,
    contains_pii=True  # â† PII Flag
))
```

### 4. Access Auditing

```python
lake.handle_audit_access(AuditAccessCommand(
    lake_id="lake-001",
    accessor_id="analyst-123",
    dataset_id="customer-data",
    action=AccessAction.QUERY.value,
    purpose="Marketing analysis"
))
```

## Event Sourcing Benefits

### 1. VollstÃ¤ndige Historie

Alle Ã„nderungen werden als Events gespeichert:
```python
events = event_store.load("lake-001")
# â†’ [DataLakeCreated, DatasetIngested, AccessAudited, ...]
```

### 2. Audit-Trail

DSGVO-konforme Nachverfolgung aller Aktionen:
```python
# Wer hat wann was gemacht?
for event in events:
    print(f"{event.timestamp}: {event.type}")
```

### 3. Snapshot-Optimierung

Bei groÃŸen Event-Streams:
```python
snapshot = Snapshot(
    aggregate_id="lake-001",
    version=1000,
    state=lake.to_dict()
)
event_store.save_snapshot(snapshot)

# Laden optimiert:
snapshot = event_store.load_snapshot("lake-001")
remaining = event_store.load("lake-001", from_version=snapshot.version)
```

## Integration in eigene Projekte

### 1. Eigene Commands definieren

```python
class YourCommand(Command):
    def __init__(self, param1: str, param2: int, **kwargs):
        super().__init__(**kwargs)
        # NICHT mit self.param = param, da frozen!
```

### 2. Eigene Events definieren

```python
class YourEvent(Event):
    def __init__(self, data: str, **kwargs):
        super().__init__(**kwargs)
        # Speichere als Attribute
```

### 3. Eigenes Aggregate erstellen

```python
from orchestrix.core.aggregate import AggregateRoot

class YourAggregate(AggregateRoot):
    def handle_command(self, cmd):
        event = YourEvent(data=cmd.data)
        self._apply_event(event)  # â† _apply_event nutzen!
    
    def _when_your_event(self, event: YourEvent):  # â† _when_ prefix!
        # State-Update hier
        pass
```

## Best Practices

1. **Immutable Events** - Events niemals Ã¤ndern
2. **Event Naming** - Vergangene Zeit (Created, Updated, Deleted)
3. **State in Aggregate** - Nur im Aggregate, nie in Events
4. **Validation** - Vor Event-Erstellung validieren
5. **Snapshots** - Bei > 100 Events pro Aggregate

## Weitere Beispiele

- `/examples/notifications.py` - Retry-Logic mit Dead Letter Queue
- `/examples/lakehouse_anonymization.py` - Data Anonymization
- `/tests/` - Umfangreiche Test-Suites

## Lizenz

Siehe LICENSE file.
